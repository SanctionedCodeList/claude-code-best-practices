Tl;dr: We need to put a coding agent in the hands of every lawyer (and staff member) at the firm as fast as possible. Doing this will bring extraordinary new opportunities, and failure to do so may bring with it harmful consequences.

Team,

It’s been a while since I’ve written an AI update, but it’s time for another one. Honestly, since my last one in the middle of this year, pace has simply been too frantic for me to sit down and write something that isn’t obsolete by the end of the week. But fortunately the holidays both give me time to collect my thoughts, and with most in the AI community on break as well, we’re probably in a week-long pause in AI news. So with that, here begins a relatively long letter with my existing thoughts.

**Coding Agents Are The Future**

As I look over the last year, my biggest takeaway is pretty simple. **The future of all knowledge work \- including law \- looks a lot like Claude Code**. In case you haven’t heard of it, Claude Code[^1] is a computer terminal application created by Anthropic that was initially intended to help software developers to use AI in their job. Claude Code came out about a year ago, and has been fun to play with, but with the release of Claude Opus 4.5 last month, it has truly been transformative. But don’t take it from me \- take it from [Andrej Karpathy](https://en.wikipedia.org/wiki/Andrej_Karpathy). You may not know him, but he was a founding member of OpenAI, and head of AI for Tesla for many years. He was one of Time Magazine’s top 100 most influential people in AI for 2024\. Here’s what he has to say about coding agents at the end of 2025:

*I've never felt this much behind as a programmer. The profession is being dramatically refactored as the bits contributed by the programmer are increasingly sparse and between. I have a sense that I could be 10X more powerful if I just properly string together what has become available over the last \~year and a failure to claim the boost feels decidedly like skill issue. There's a new programmable layer of abstraction to master . . . and a need to build an all-encompassing mental model for strengths and pitfalls of fundamentally stochastic, fallible, unintelligible and changing entities suddenly intermingled with what used to be good old fashioned engineering.* 

***Clearly some powerful alien tool was handed around except it comes with no manual and everyone has to figure out how to hold it and operate it, while the resulting magnitude 9 earthquake is rocking the profession**. Roll up your sleeves to not fall behind.*

[@karpathy, Dec. 26, 2025\.](https://x.com/karpathy/status/2004607146781278521?s=20)  
If Andrej can’t keep up, what hope do we have? And yet, if we don’t try, what hope do we have then? 

Beyond Prompt and Context Engineering

So what makes him so blown away by these capabilities? It’s actually quite simple. It does all the same things that ChatGPT, [Claude.ai](http://Claude.ai), or Gemini can do in your web browser, but adds to it two critical additional capabilities: (1) the ability to read and write files on your computer, and (2) the ability to run terminal commands on your computer. These two things unlock extraordinary capability.

What this means is that you can have a working folder with a bunch of files (docket sheets, discovery materials, case law, whatever), and Claude Code is smart enough to search through that and find the relevant materials to help provide a better answer. No need to manually upload a few documents for every question, or build some bespoke “workspace” thing in a web ui. And it can do more than just answer your question \- it can write to a file and create memos, documents, code, whatever. It’s literally just files on your computer. Dead simple. 

Maybe the best way to think about this is that 2022-24 were the years of “prompt engineering” where you had to be really clever in your prompts to get the best out of AI. Then AI got good enough that prompt engineering didn’t really matter, and 2024-2025 were the years that feeding the AI the right context (docs, etc.) at the right time was important \- which led to things like Retrieval Augmented Generation (RAG) and “context engineering.” Claude Code obsoletes both \- it is smart enough to figure out what you’re asking for, and (if you put it in the right folder) smart enough to find what it needs to answer that question. 

How do you train an Agent?  
Now, all the above probably sounds too good to be true. And there are parts that are \- even as great as Claude Opus 4.5 is (the current frontier model powering it is), it has a “house style” that doesn’t feel natural to lawyers. And while it starts at a reasonably high level of capability, there are lots of places where we add value because we know how to do tasks efficiently, and using AI agents \- even when they work \- sometimes feels like watching it reinvent the wheel every time.

Enter the concept of [Skills](https://agentskills.io/home), a new open standard supported by Claude Code and others. At its most basic level, it’s just a plain text file full of whatever guidance you want to give an AI agent about how tasks should be done \- what inputs do you need? What steps should you perform? What examples should you follow? Skills allow you to ask an agent to do some specific task, and it can discover the skill, open it, and follow those instructions to give you results following your methods and outputs in your style. 

But how do you make a skill? With a skill of course\! Anthropic provides [a skill-creator skill](https://github.com/anthropics/skills/blob/main/skills/skill-creator/SKILL.md) that provides instructions to AI about how to make new skills. A pattern I follow that has been extraordinarily effective is to walk an agent through how to do some task the first time \- correcting it as I go, and then when I’m happy with the result, asking it to invoke that skill, and either create a new skill or update an existing skill based on what it’s learned in the process. In that way, the Skills I use every day can grow and extend based on my usage rather than having to set aside time just to make skills. Some of my most used include a skill on legal style \- which started as a summary of Wydick’s *Plain English for Lawyers* and Brian Garner’s *The Winning Brief*, and now includes detailed style guides for persuasive briefing, internal objective analysis, client communications, and thought leadership. I also have a “legal research” skill that provides custom tools to access a wide array of legal resources (like every API the USPTO provides, the Federal Register, US Code, SEC’s EDGAR, USITC EDIS, CourtListener, etc.). 

A Nation of Lawyers in a Data Center

But that’s not where the amazing features end. Claude Code can also launch subagents. That is, sometimes there are tasks that can be run in parallel. Imagine reviewing a complaint, coming up with a list of legal research topics. Rather than handing them off to associates, or opening a dozen browser tabs, Claude Code can launch clones of itself for each task and do them independently, each writing its own research memo which can then be synthesized and read by the original agent. That is, Claude Code is not just like having a digital employee on call \- it’s like having a digital manager that can create employees for it to manage on-demand. In the hands of a skilled user, this doesn’t just let that user do one task faster, but manage many tasks at once. As someone who does this every day, it feels a bit like a superpower.

Hallucinations and Verification  
One other lesson I’ve learned this year is that spending time on verifying AI outputs is a waste of time. Often it is an *essential* and *ethically obligated* task, but that doesn’t make it any less wasteful. Our goal should be to put humans in the role of quarterback of AI systems, not as verifiers. Checking things is a computer task \- not a human task. And one of our top AI priorities *must* be getting out of the verification business safely. 

But how do we do that? We have to find ways to verify faster and verify with a degree of automation that provides programmatic guarantees of correctness independent of AI. Here’s an example I’ve used that has worked well. First, I give Claude Code access to the draft and a folder with all the documents I rely upon. Claude Code then produces a spreadsheet that includes columns for supported statements, supporting citation, and a quotation from the source document that supports the statement. Then, I use non-AI software to confirm that the quote appears in the source document, adding a fourth column to that same sheet. That gives me a companion document to review with my brief, and all I have to confirm is that (1) it identified all cited statements, and (2) that within that spreadsheet the identified quotes support those statements. That is working with AI to give me programmatic guarantees of accuracy \- which is what we’re after. 

Do I have to learn programming to do this?

The short answer is no. Let me tell you a brief story about my wife. She works as a director of medical affairs for a medical device company. One of her jobs is managing a registry of real-world patient data reported from doctor’s offices that provide detailed (but anonymized) information about how their product is performing in the field. She used to employ a biostatistician to help analyze this data and support academic publishing based on conclusions drawn from this data. I gave her a 1-day introduction to Claude Code. She doesn’t need a biostatistician anymore. Indeed, now analyses that used to keep her busy for a week can be done in less than an hour. And one of her big victories this year is that her company obtained a label extension from the FDA to allow the device to be used in new patient populations *based entirely upon analysis performed by Claude Code*. Not only that, but she back-tested Claude’s analyses against a few years of research she had done \- and the results were perfect matches. 

The same can be said for lawyers. Now, there are advantages if you have any background in coding at all. Some of them are simple \- like not being intimidated by terminal commands. Others are more profound \- like being able to think in algorithmic and procedural terms about how to solve problems in your job. But these come not from understanding any programming language, but from the experience of knowing how to solve problems with computers in detail. Indeed, I have not written any meaningful code in the last 12 months \- Claude has done that for me.

What about security and confidentiality?

Those are excellent questions. Indeed, those are *the* questions we must ask and answer as a law firm. And Baker Botts has a lawyer-facing answer \- our Generative AI policy, which essentially says nothing that has any obligation of confidentiality should go in an AI system not approved by our IT department. 

But let’s dig a little deeper and answer these at a level that shows the real risks and challenges here. There are good answers. Indeed, the coding agent paradigm actually keeps a lot of the control over data in our hands. The best way to think about confidentiality and security are really about three things: (1) AI traces, (2) Baker Botts data, and (3) the internet, and how those three things interact (or don’t) with one another. So let’s walk through them:

*AI Traces*  
This is the information actually input and output from the AI model itself. When lawyers are asked what to worry about with AI, this is suspect \#1. When we worry about people “training on our data,” what we are talking about is where and how AI traces are handled. And because AI models require *very* powerful computers, there is no pragmatic way for most businesses to use AI without all the information submitted to, and retrieved from, an AI model passing through the AI model provider’s hands.[^2] Which means AI providers can \- and often do \- log this information. AI traces at the model provider’s level are basically handled in one of three ways:

* **Public / Free Apps** **on “Improve our Products” Terms** \- Most AI products have a free and publicly available tier. This is the wild west. AI companies can and do use this information to train and improve their systems \- which means information you provide is provided without any obligation of confidentiality and it could show up in later outputs provided by models trained on that data. These are off-limits for confidential data.  
    
* **Paid / Business Apps on Confidential “Safety” Terms** \- Most AI products on paid tiers instead have confidential terms with a “safety” carve-out. Consumer-facing products are a mix of “Safety” and “Improve our Products” terms, but most B2B products are on these terms. Under these terms, any information submitted will not be used for training, and held as confidential information \- with one exception. To prevent powerful AI systems from being used for nefarious purposes (cybersecurity incidents, developing CBRN weapons, etc), your inputs and outputs are subject to review by mostly automated systems solely for the purposes of abuse monitoring. From a legal perspective, it is unclear whether this is adequate to preserve attorney-client privilege and/or confidentiality. I think it probably does, and can come up with good arguments for it \- but I can’t point to a case that says so.   
    
* **Paid / Business Apps on Zero Data Retention (“ZDR”) Terms** \- All the frontier AI providers additionally offer what are referred to as “ZDR” terms. These terms are exactly what they sound like. The model provider processes your input, provides an output, and does not retain any of that information. It exists solely in volatile memory for as long as it takes to generate output. 

From a purely legal perspective, which of these can be used with attorney-client, protective order, or client confidential information (setting aside any specific OCG or PO terms)? Almost certainly ZDR terms are sufficient, and “improve our products” are not. And we are hopefully only a few court cases away from the adequacy of “safety” terms (maybe we can get another AmLaw 100 firm to volunteer to test it?). 

*Baker Botts Data & The Internet*  
How you deal with these data sources is ultimately a question of inputs and outputs. With something like an AI agent, when you hand it your data, there are two places you need to worry about that data going. One is in the AI traces (discussed above). The other is that the agent could independently send that information to third parties. For example, if the agent had access to an email tool, it could attach a file to an email and send it along. And when you provide it access to the internet, you provide another source it could send data to, and potentially a way for malicious actors to inject harmful information into your AI agent \- like instructions to do something it shouldn’t. 

So how do you defend against these risks? Good perimeter defenses. That is, reinforcing the key points where the agent touches external systems. Claude Code (and agents like it) have built-in permissioning systems where for potentially destructive or harmful actions, you must provide human confirmation before it takes action. But any regular user will tell you that it happens frequently enough that no one reviews it in detail. So instead, you need hard limits around what it can do. For example, an allow/block list of websites or network systems it can interact with. Or for email and other messaging systems, only giving it the ability to provide drafts but not to send \- so you have to go to another system to confirm. But these are solvable problems.

Build vs. Buy  
Of course, while Claude Code can be good at legal tasks, it’s also a *coding agent\!* And can write code. Much of the most amazing discourse over Claude Code is from the software development community, many of which are predicting essentially the end of coding as a human task in the next six months. Put simply, when it comes to software, writing code is no longer the bottleneck. If skill at writing code is no longer a bottleneck, what still is? What guides the build vs. buy decision? The way I see it, these are the key bottlenecks:

1. **Ideas** \- Who is thinking about what *should* be built or exist?   
     
2. **Fixed Costs** \- Are there unavoidable fixed costs that don’t scale with user count that make vendors better suited to run the actual silicon where it runs.  
     
3. **Data & Standards Lock-In** \- Are there pieces of the system that have hard requirements to comply with standards that require certification or licensure, or proprietary data that requires licensing?   
     
4. **Security** \- What is the process for ensuring code is securely designed and run on systems with good security practices?

In a modern cloud environment, \#2 is almost never an actual limitation. \#3 can be tricky, but open standards, and internet access often bridge the gap.. And \#4 is the most practical one today \- but one where AI continues to make progress. Security review and maintenance may \- in the very near future \- be as effectively done by AI as coding is.

Put that all together, and the logic behind building rather than buying becomes far more powerful. Add to it that internally-built systems are inherently more flexible and easier to integrate with others \- because you control the code. 

[^1]:  OpenAI has a similar product called Codex CLI, and Google has one called Gemini CLI. There are others (OpenCoder), etc., but most of my time recently has been on Claude Code. So that’s the focus here.

[^2]:  Setting aside companies of adequate size and sophistication that can run open weights models either on-prem or in VPC environments. 